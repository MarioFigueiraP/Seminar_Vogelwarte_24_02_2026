<!DOCTYPE html>
<html lang="en"><head>
<script src="Seminar_Vogelwarte_24_02_2026_files/libs/clipboard/clipboard.min.js"></script>
<script src="Seminar_Vogelwarte_24_02_2026_files/libs/quarto-html/tabby.min.js"></script>
<script src="Seminar_Vogelwarte_24_02_2026_files/libs/quarto-html/popper.min.js"></script>
<script src="Seminar_Vogelwarte_24_02_2026_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Seminar_Vogelwarte_24_02_2026_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Seminar_Vogelwarte_24_02_2026_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="Seminar_Vogelwarte_24_02_2026_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="dcterms.date" content="2026-02-26">
  <title>seminar_vogelwarte_24_02_2026</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/dist/theme/quarto-b29769a5f4da14c4a86ec3187aac90f6.css">
  <link href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="section" class="slide level2" data-background-image="./Figures/Title_Page_Thesis.png">
<h2></h2>
</section>
<section>
<section id="preamble" class="title-slide slide level1 center">
<h1>Preamble</h1>

</section>
<section id="dealing-with-high-complexity" class="slide level2 smaller">
<h2>Dealing with high complexity</h2>
<p>In highly complex settings, two broad frameworks are commonly used:</p>
<div class="fragment-container">
<ul>
<span class="fragment fade-in">
<li>
<strong>Sequential (or recursive) approaches</strong>, which enable information sharing across models and allow for step-by-step updating as new data become available.
</li>
<p> <img data-src="./Figures/Sequential_Inference.png" class="fragment fade-in-then-out absolute" style="top: 250px; right: 50px; width: 1200px; height: 420px; "> <img data-src="./Figures/Sequential_Inference.png" class="fragment fade-in absolute" style="top: 465px; left: 50px; width: 500px; height: 210px; "></p>
<span class="fragment fade-in">
<li>
<strong>Distributed approaches</strong>, which address privacy concerns and make it possible to analyze very large datasets by distributing computation and overcoming scalability challenges.
</li>
</span> <img data-src="./Figures/Parallel_Inference_Extended.png" class="fragment fade-in-then-out absolute" style="top: 60px; right: 60px; width: 2150px; "> <img data-src="./Figures/Parallel_Inference.png" class="fragment fade-in absolute" style="top: 400px; left: 700px; width: 410px; height: 350px; ">
</span></ul>
</div>
<div class="fragment fade-in">
<p>The developments in these topics are based on the INLA (<em>Integrated Nested Laplace Approximations</em>) method.</p>
</div>
</section></section>
<section>
<section id="outline" class="title-slide slide level1 center">
<h1>Outline</h1>

</section>
<section id="section-1" class="slide level2 smaller">
<h2></h2>
<ul>
<li>Introduction
<ul>
<li>Bayesian Inference</li>
<li>INLA</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>Spatial and Spatio-Temporal Models and Data</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Dealing with high complexity</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Updating model information
<ul>
<li>Bayesian Feedback</li>
<li>Sequential Consensus</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Distributed and Recursive Inference
<ul>
<li>Distributed Inference (DI)</li>
<li>Recursive Inference (RI)</li>
<li>Partitioning model structures</li>
<li>Combining Distributed and Recursive Inference</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Conclusions</li>
</ul>
</div>
</section></section>
<section>
<section id="introduction" class="title-slide slide level1 center">
<h1>Introduction</h1>

</section>
<section id="bayesian-inference" class="slide level2">
<h2>Bayesian inference</h2>
<p>Bayesian inference is one the two main approaches to perform statistical inference. It is grounded in the Bayes’ theorem:</p>
<p><span class="math display">\[\begin{equation}
\pi(\boldsymbol\xi, \mathbf{y}) = \pi(\mathbf{y} \mid \boldsymbol\xi) \pi(\boldsymbol\xi) = \pi(\boldsymbol\xi \mid \mathbf{y}) \pi(\mathbf{y}) \Rightarrow \boxed{\textcolor{red}{\pi(\boldsymbol\xi \mid \mathbf{y})} = \frac{\textcolor{green}{\pi(\mathbf{y} \mid \boldsymbol\xi)} \textcolor{blue}{\pi(\boldsymbol\xi)}}{\textcolor{orange}{\pi(\mathbf{y})}}},
\end{equation}\]</span> where <span class="math inline">\(\pi(\xi \mid \mathbf{y})\)</span> is the <a style="color: red; font-synthesis-weight: auto;" href="">posterior distribution</a> of the model parameters <span class="math inline">\(\xi\)</span>, <span class="math inline">\(\pi(\mathbf{y} \mid \boldsymbol\xi)\)</span> is the <strong>likelihood</strong> (or <a style="color: green; font-synthesis-weight: auto;" href="">observational model</a>), <span class="math inline">\(\pi(\xi)\)</span> is the <a style="color: blue; font-synthesis-weight: auto;" href="">prior distribution</a> of the model parameters and <span class="math inline">\(\pi(\mathbf{y})\)</span> is the <a style="color: orange; font-synthesis-weight: auto;" href="">marginal likelihood</a>, <span class="math inline">\(\pi(\mathbf{y}) = \int_{\xi} \pi(\mathbf{y} \mid \boldsymbol\xi)\pi(\boldsymbol\xi)d\boldsymbol\xi\)</span>.</p>
<div class="fragment">
<p>However, <strong>behind such an apparently simple expression lies a remarkably rich and elegant collection of perspectives</strong>, together with the theoretical foundations that constitute the underlying substrate of this theorem.</p>
</div>
</section>
<section id="bayesian-inference-1" class="slide level2">
<h2>Bayesian inference</h2>
<p>It may seems that a straight possibility is to just solve analitically the previous equation, but nothing fardest from reality, as only some simple (toy) models can be truelly solve in this way, due to the difficulty of calculate the marginal likelihood <span class="math inline">\(\pi(\mathbf{y})\)</span>.</p>
<div class="fragment">
<p>Therefore, within the Bayesian framework, several methods are available for inferring the parameters <span class="math inline">\(\boldsymbol{\xi}\)</span>, including:</p>
<ul>
<li>Monte Carlo (MC) methods,</li>
<li>Markov Chain Monte Carlo (MCMC),</li>
<li>Adaptive Gaussian–Hermite Quadrature (AGHQ),</li>
<li>Variational Inference (VI), and</li>
<li><strong>Integrated Nested Laplace Approximations</strong> (INLA).</li>
</ul>
</div>
</section>
<section id="inla" class="slide level2">
<h2>INLA</h2>
<ul>
<li>INLA is an ad hoc method, focused on obtaining the marginal posterior distributions of the latent field (<span class="math inline">\(x_i\)</span>) and the hyperparameters (<span class="math inline">\(\theta_i\)</span>):</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{array}{c}
\pi(\theta_i \mid \mathbf{y}) = \int_{\boldsymbol\theta_{-i}\in \boldsymbol\Theta}\int_{\mathbf{x} \in \mathbf{X}} \pi(\mathbf{x}, \boldsymbol\theta \mid \mathbf{y}) d\mathbf{x}d\boldsymbol\theta_{-i}, \\
\pi(x_i \mid \mathbf{y}) = \int_{\boldsymbol\theta \in \boldsymbol\Theta} \int_{\mathbf{x}_{-i} \in \mathbf{X}} \pi(\mathbf{x}, \boldsymbol\theta \mid \mathbf{y}) d\mathbf{x}_{-i}d\boldsymbol\theta.
\end{array}
\end{equation}\]</span></p>
<ul>
<li>It is focused for hierarchical models with latent Gaussian fields. Therefore, we usually have the following structure for models in INLA:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{array}{c}
y_i \mid \mathbf{x}, \boldsymbol\theta \sim \ell(\mu_i, \boldsymbol\theta_1),\\
\mathbb{E}(y_i) = \mu_i = g^{-1}(\eta_i)=g^{-1}(\mathbf{A}_i\mathbf{x}),\\
\mathbf{x} \mid \boldsymbol\theta_2 \sim GMRF(\boldsymbol\mu, \mathbf{Q}(\boldsymbol\theta_2)),\\
\boldsymbol\theta \sim \pi(\boldsymbol\theta_1, \boldsymbol\theta_2).
\end{array}
\end{equation}\]</span></p>
</section>
<section id="inla-1" class="slide level2">
<h2>INLA</h2>
<ul>
<li>To achieve this INLA uses a deterministic exploration with a quadrature rule to compute the marginal of the latent field, along with another key strategies to compute the marginal of the hyperparameters. For the joint marginal of the hyperparameters INLA uses the following functional form, evaluated at the mode of conditional posterior of the latent field:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\tilde{\pi}(\boldsymbol\theta \mid \mathbf{y}) = \left.\frac{\pi(\mathbf{y} \mid \mathbf{x}, \boldsymbol\theta) \pi(\mathbf{x} \mid \boldsymbol\theta) \pi(\boldsymbol\theta)}{\tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y})}\right|_{\mathbf{x} = \mathbf{x}^*(\boldsymbol\theta)}.
\end{equation}\]</span></p>
<ul>
<li>And for the marginal of the latent field INLA uses a quadrature rule to approximate the integration:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\pi(x_i \mid \mathbf{y}) = \iint\pi(x \mid \boldsymbol\theta, \mathbf{y}) \pi(\boldsymbol\theta \mid \mathbf{y}) d\mathbf{x}_{-i}d\boldsymbol\theta \approx \sum_{k=1}^K \tilde{\pi}(x_i \mid \boldsymbol\theta^k, \mathbf{y}) \tilde{\pi}(\boldsymbol\theta^k \mid \mathbf{y})\Delta_k.
\end{equation}\]</span></p>
</section>
<section id="inla-2" class="slide level2 smaller">
<h2>INLA</h2>
<h3 id="conditional-posterior-of-the-latent-field">Conditional posterior of the latent field</h3>
<p>The conditional posterior of the latent field is computed as the Gaussian approximation at the mode of the true conditional posterior:</p>
<p><span class="math display">\[\begin{equation}
\tilde{\pi}_G(\mathbf{x} \mid \mathbf{y}, \boldsymbol\theta) = C(\mathbf{y}, \boldsymbol\theta) \exp\left[ (\mathbf{x} - \mu(\mathbf{y}, \boldsymbol\theta))^\top \mathbf{Q}(\mathbf{y}, \boldsymbol\theta) (\mathbf{x} - \mu(\mathbf{y}, \boldsymbol\theta)) \right],
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mu(\mathbf{y}, \boldsymbol\theta)\)</span> is the mean of the Gaussian approximation at the mode (<span class="math inline">\(\mathbf{x}^*\)</span>) of the true posterior, and <span class="math inline">\(\mathbf{Q}(\mathbf{y}, \boldsymbol\theta)\)</span> is the negative Hessian at the mode.</p>
<ul>
<li>Mean of the Gaussian approximation of the conditional posterior:</li>
</ul>
<p><span class="math display">\[
\mu(\mathbf{y}, \boldsymbol\theta) = \mathbf{x}^*= \arg\min_{x}\left\{ - \log\pi(\mathbf{y} \mid \mathbf{x}, \boldsymbol\theta) - \log\pi(\mathbf{x} \mid \boldsymbol\theta) \right\}.
\]</span></p>
<ul>
<li>Precision matrix of the Gaussian approximation:</li>
</ul>
<p><span class="math display">\[
\mathbf{Q}(\mathbf{y}, \boldsymbol\theta) = \mathbf{Q}(\boldsymbol\theta) - \left[\mathbf{A}^\top \frac{\partial^2 \log\pi(\mathbf{y} \mid \boldsymbol\eta, \boldsymbol\theta)}{\partial \boldsymbol\eta^2} \mathbf{A} \right]_{\boldsymbol\eta = \mathbf{A}\mathbf{x}^*}.
\]</span></p>
</section>
<section id="inla-3" class="slide level2 smaller">
<h2>INLA</h2>
<h3 id="joint-marginal-posterior-of-hyperparameters">Joint marginal posterior of hyperparameters</h3>
<p>The joint marginal posterior of the hyperparameters is one of the main components that we need in the INLA approach, as we need to compute the modal configuration of the joint marginal posterior of the hyperparamters to build a integration scheme for the marginal of the latent field.</p>
<p>The formula to compute the posterior (and the configuration) of the joint marginal posterior is, as stated before, <span class="math display">\[
\tilde{\pi}(\boldsymbol\theta \mid \mathbf{y}) = \left.\frac{\pi(\mathbf{y} \mid \mathbf{x}, \boldsymbol\theta) \pi(\mathbf{x} \mid \boldsymbol\theta) \pi(\boldsymbol\theta)}{\tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y})}\right|_{\mathbf{x} = \mathbf{x}^*(\boldsymbol\theta)}.
\]</span></p>
<p>Therefore, using this equation we can compute the modal configuration by <em>finite differences</em>, as the analytical expression for this equation is not generally available:</p>
<p><span class="math display">\[
\boldsymbol\theta^* = \arg \min_{\boldsymbol\theta} \left\{-\log \pi(\mathbf{y}\mid \mathbf{x}, \boldsymbol\theta) -\log\pi(\mathbf{x} \mid \boldsymbol\theta) -\log\pi(\boldsymbol\theta) + \tilde{\pi}_G(\mathbf{x}\mid \mathbf{y}, \boldsymbol\theta) \right\}_{\mathbf{x}=\mathbf{x}^*(\boldsymbol\theta)},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}^*\)</span> is the mode of the conditional posterior of the latent field for each <span class="math inline">\(\boldsymbol\theta\)</span> set of values.</p>
</section>
<section id="inla-4" class="slide level2 smaller">
<h2>INLA</h2>
<h3 id="marginal-posterior-of-the-latent-field">Marginal posterior of the latent field</h3>
<p>The marginal posterior distribution for each latent field node (<span class="math inline">\(\pi(x_i \mid \mathbf{y})\)</span>) is computed by the following expression: <span class="math display">\[
\tilde{\pi}(x_i \mid \mathbf{y}) = \sum_{k=1}^K \tilde{\pi}(x_i \mid \boldsymbol\theta^k, \mathbf{y}) \tilde{\pi}(\boldsymbol\theta^k \mid \mathbf{y}) \Delta_k,
\]</span> where <span class="math inline">\(\Delta_k\)</span> is the weight from the integration scheme for each support point <span class="math inline">\(\theta^k\)</span>, <span class="math inline">\(\tilde{\pi}(\boldsymbol\theta^k \mid \mathbf{y})\)</span> is the joint marginal posterior computed previously, and <span class="math inline">\(\tilde{\pi}(x_i \mid \mathbf{y}, \boldsymbol\theta^k)\)</span> is the most delicate quantity to calculate.</p>
<p>The following strategies are proposed to compute this quantity in the INLA methodology:</p>
<ul>
<li>marginalized the Gaussian approximation of the conditional posterior at the mode,</li>
<li>apply a Laplace approximation for each node,</li>
<li>use a simplified Laplace approximation (a Taylor expansion of 3rd order), or</li>
<li>use the Gaussian approximation with a Variational Bayes (VB) correction of the mean.</li>
</ul>
<p>Once we have computed all these quantities we can calculate goodness-of-fit measures like DIC, WAIC, CPO, etc.</p>
</section>
<section id="inla-5" class="slide level2 smaller">
<h2>INLA</h2>
<h3 id="marginal-posterior-of-hyperparameters">Marginal posterior of hyperparameters</h3>
<p>The marginal posterior of the hyperparameters can be computed following several different strategies. Some of them are:</p>
<ul>
<li>Marginalized a Gaussian approximation around the mode of the joint marginal posterior.</li>
<li>Numerical integration using a exploration grid and interpolation.
<ul>
<li>Direct interpolation: <span class="math inline">\(\pi(\theta_j \mid \mathbf{y}) = \int_{\boldsymbol\theta_{-j}\in \boldsymbol\Theta} I(\boldsymbol\theta)d\boldsymbol\theta_{-j}\)</span>, or</li>
<li>Interpolation using an asymmetric Gaussian apprximation.</li>
</ul></li>
<li>Numerical integration-free algorithm (default in <code>R-INLA</code>).</li>
</ul>
<p>The default approach (<em>numerical integration-free algorithm</em>) levarages an asymmetric Gaussian approach, where</p>
<p><span class="math display">\[
\tilde{\pi}(\theta_j \mid \mathbf{y}) \propto \left\{
\begin{array}{l}
\exp\left(-\frac{1}{2(\sigma^+_j)^2}\theta_j^2 \right) \\
\exp\left(-\frac{1}{2(\sigma^-_j)^2}\theta_j^2 \right)
\end{array}
\right.
\]</span></p>
<p>and leverages the following lemma: <span class="math inline">\(-\frac{1}{2}(\theta_j, \mathbb{E}(\boldsymbol\theta_{-j} \mid \theta_j))^\top \boldsymbol\Sigma^{-1}_{\boldsymbol\theta} (\theta_j, \mathbb{E}(\boldsymbol\theta_{-j} \mid \theta_j)) = -\frac{1}{2}\frac{\theta_{j}^2}{\Sigma_{jj}}\)</span>.</p>
</section></section>
<section>
<section id="spatial-and-spatio-temporal-models-and-data" class="title-slide slide level1 center">
<h1>Spatial and Spatio-Temporal Models and Data</h1>
<ul>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Introduction
</li>
<li style="color: red; font-synthesis-weight: auto;;">
<strong>Spatial and Spatio-Temporal Models and Data</strong>
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Dealing with high complexity
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Updating model information
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Distributed and Recursive Inference
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Implementations
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Conclusions
</li>
</ul>
</section>
<section id="spatial-and-spatio-temporal-models-and-data-1" class="slide level2 smaller">
<h2>Spatial and Spatio-Temporal Models and Data</h2>
<p>Spatial and spatio-temporal models incorporate spatial (or spatio-temporal) components into the model structure to assess how these components influence the inference procedure and what information related to the spatial (or spatio-temporal) dependence they capture.</p>
<p>In classical spatial statistics are three types of spatial data:</p>
<ul>
<li>areal data,</li>
<li>geostatistical data, and</li>
<li>point pattern data (point processes).</li>
</ul>
<p><img data-src="./Figures/Areal_Data.png" class="absolute" style="top: 410px; left: 0px; width: 450px; height: 300px; " alt="Areal Data"> <img data-src="./Figures/Geostatistical_Data.png" class="absolute" style="top: 410px; left: 430px; width: 450px; height: 300px; " alt="Geostatistical Data"> <img data-src="./Figures/Point_Pattern_Data.png" class="absolute" style="top: 410px; left: 850px; width: 450px; height: 300px; " alt="Pattern Data"></p>
</section>
<section id="spatial-and-spatio-temporal-models-and-data-2" class="slide level2 smaller">
<h2>Spatial and Spatio-Temporal Models and Data</h2>
<p>For these three data types (areal, geostatistical, and point pattern data), different modeling approaches exist in which latent components (prior distributions) play a central role, although they may be embedded differently across models. In particular, these models can be characterized as follows:</p>
<ul>
<li><strong>Areal model</strong>:</li>
</ul>
<p><span class="math display">\[
\mathbf{u} \sim \text{GMRF}(\boldsymbol\mu, \mathbf{Q} = f(\mathbf{D,W,\boldsymbol\theta})), \quad \mathbf{Q}_{st} = \mathbf{Q}_s \otimes \mathbf{Q}_t \; \text{or} \; \mathbf{Q}_{st} = \mathbf{Q}_t \otimes \mathbf{Q}_s.
\]</span></p>
<ul>
<li><strong>Geostatistical model</strong>:</li>
</ul>
<p><span class="math display">\[
\mathbf{Q} = \tau^2 (\kappa^4 \mathbf{C} + 2\kappa^2\mathbf{G} + \mathbf{G}\mathbf{C}^{-1}\mathbf{G}).
\]</span></p>
<ul>
<li><strong>Point processes</strong>:</li>
</ul>
<p><span class="math display">\[
\log \pi(\mathbf{y} \mid \mathbf{x}, \boldsymbol\theta) = |\boldsymbol\Omega|-\int_{\mathbf{s} \in \boldsymbol\Omega} \lambda(s \mid \mathbf{x}, \boldsymbol\theta) + \sum_{i=1}^n \log(\lambda(s_i \mid \mathbf{x}, \boldsymbol\theta)).
\]</span></p>
</section>
<section id="spatial-and-spatio-temporal-models-and-data-3" class="slide level2 smaller">
<h2>Spatial and Spatio-Temporal Models and Data</h2>
<h3 id="what-are-areal-data-and-how-are-they-modeled">What are areal data, and how are they modeled?</h3>
<ul>
<li><p>The study region is partitioned into <strong>non-overlapping areas</strong> (e.g., districts, administrative regions, or grid cells). For example, continuous observations may be <strong>aggregated by area</strong>.</p></li>
<li><p>Typical applications include:</p>
<ul>
<li>Disease mapping (health outcomes aggregated by region)</li>
<li>Ecology (abundance and occupancy)</li>
<li>Socio-economic analysis (e.g., census data)</li>
</ul></li>
<li><p>To model <strong>spatial dependence</strong>, areal models—also interpretable as network or graphical models—often use:</p>
<ul>
<li><strong>Conditional Autoregressive (CAR) models</strong></li>
<li><strong>Intrinsic CAR (ICAR) models</strong></li>
</ul></li>
</ul>
<p>These approaches capture <strong>spatial dependence between neighboring areas</strong>.</p>
</section>
<section id="spatial-and-spatio-temporal-models-and-data-4" class="slide level2 smaller">
<h2>Spatial and Spatio-Temporal Models and Data</h2>
<h3 id="conditional-autorregresive-car-structure">Conditional Autorregresive (CAR) Structure</h3>
<p>For each area <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
x_i \mid x_{-i} \sim
\mathcal{N}\left(
\mu_i + \sum_{j \sim i} w_{ij}(x_j - \mu_j),\;
\sigma_i^2
\right)
\]</span></p>
<ul>
<li><span class="math inline">\(j \sim i\)</span>: neighbors of area <span class="math inline">\(i\)</span>, i.e., <span class="math inline">\(j \in \mathcal{R}(i)\)</span>.</li>
<li>The weights <span class="math inline">\(w_{ij}\)</span> define the strength of dependence and are encoded in the adjacency (neighborhood) matrix <span class="math inline">\(\mathbf{W}\)</span>.</li>
<li>The smoothing properties depend critically on how the neighborhood structure is defined.</li>
</ul>
<p>The model is specified <strong>through its full conditional distributions</strong>–hence the name <em>conditional autoregressive</em>. Using Brook’s lemma, the CAR model can be expressed in joint form:</p>
<p><span class="math display">\[
\pi(\mathbf{x}) \propto
\exp\left[
-\frac{1}{2}
(\mathbf{x}-\boldsymbol{\mu})
\mathbf{Q}
(\mathbf{x}-\boldsymbol{\mu})^\top
\right],
\]</span></p>
<p>where the precision matrix is: <span class="math inline">\(\mathbf{Q} = \tau (\mathbf{D} - \mathbf{W})\)</span>.</p>
</section>
<section id="spatial-and-spatio-temporal-models-and-data-5" class="slide level2 smaller">
<h2>Spatial and Spatio-Temporal Models and Data</h2>
<h3 id="icar-model.-problem-and-solutions">ICAR Model. Problem and solutions:</h3>
<p>The main limitation of the Intrinsic CAR (ICAR) model is that its precision matrix is not invertible (i.e., the inverse does not exist). Therefore, marginal variances and other marginal properties cannot be directly derived from the model.</p>
<ul>
<li>The matrix <span class="math inline">\(\mathbf{D} - \mathbf{W}\)</span> is <strong>singular</strong>, and therefore the distribution is <strong>improper</strong>.</li>
<li>This specification is known as the <strong>Intrinsic CAR (ICAR)</strong> model.</li>
</ul>
<p>Several approaches can be used to address this issue:</p>
<ul>
<li><p>Impose a <strong>sum-to-zero constraint</strong>: <span class="math display">\[
\mathbf{1}^\top \mathbf{x} = 0
\quad \Rightarrow \quad
\pi(\mathbf{x} \mid \mathbf{C}\mathbf{x} = \mathbf{e}).
\]</span> This yields a constrained <strong>Gaussian Markov Random Field (GMRF)</strong>.</p></li>
<li><p>Define proper versions by modifying the structure matrix <span class="math inline">\(\mathbf{Q}(\boldsymbol{\theta}, \tau) = \tau \mathbf{R}(\boldsymbol{\theta})\)</span>, for example:</p></li>
</ul>
<p><span class="math display">\[
\begin{array}{c}
\mathbf{Q} = \tau (\lambda (\mathbf{D} - \mathbf{W}) + (1-\lambda)\mathbf{I}), \quad \lambda \in (0,1), \quad \text{Leroux model}, \\
\mathbf{Q} = \tau (\mathbf{D} + d\mathbf{I} - \mathbf{W}), \quad d&gt;0, \quad \text{Proper Besag model}.
\end{array}
\]</span></p>
</section>
<section id="friction-boundaries-and-barriers" class="slide level2 smaller">
<h2>Friction, Boundaries, and Barriers</h2>
<h3 id="less-common-extensions-of-proper-car-models">Less Common Extensions of Proper CAR Models</h3>
<p>There are several extensions of the standard areal model that are less well known and less widely used. One crucial component in defining a (P)CAR model is the <strong>adjacency matrix</strong>. Instead of assigning equal weights to all neighboring areas, it is possible to introduce more flexible specifications for the weights. Some examples include:</p>
<ul>
<li><strong>Gaussian kernel weights with covariates</strong> (interpreted as friction or permeability driven by covariate differences):
<ul>
<li><span class="math inline">\(\mathbf{W}_{ij} = \exp\!\left(-\frac{1}{2}\frac{(x_i - x_j)^2}{l^2}\right)\)</span>,</li>
<li><span class="math inline">\(\mathbf{W}_{ij} = \exp\!\left(-\frac{1}{2}\frac{||\mathbf{X}_i - \mathbf{X}_j||^2}{l^2}\right)\)</span>,</li>
<li><span class="math inline">\(\mathbf{W}_{ij} = \displaystyle \prod_{k=1}^K\exp\!\left(-\frac{1}{2}\frac{||x_{ki} - x_{kj}||^2}{l_k^2}\right).\)</span></li>
</ul></li>
<li><strong>Barrier models</strong> (fixed or adaptively varying permeability):
<ul>
<li><span class="math inline">\(\mathbf{W}_{ij} = \exp(-\lambda_B)\)</span>, if <span class="math inline">\(j \in \mathcal{B}\)</span>; otherwise <span class="math inline">\(\mathbf{W}_{ij} = 1\)</span> or <span class="math inline">\(0\)</span> (fixed).</li>
<li><span class="math inline">\(\mathbf{W}_{ij} = \exp(-\lambda_B(t))\)</span>, if <span class="math inline">\(j \in \mathcal{B}\)</span>; otherwise <span class="math inline">\(\mathbf{W}_{ij} = 1\)</span> or <span class="math inline">\(0\)</span> (adaptavely).</li>
</ul></li>
<li><strong>Model combinations or combined regularization</strong>:
<ul>
<li><span class="math inline">\(\sqrt{1-\phi} \cdot \mathbf{x}_{L} + \sqrt{\phi} \cdot \mathbf{x}_{ML}\)</span> (it would require an informative prior in some hyparameters of <span class="math inline">\(\mathbf{x}_{ML}\)</span> to avoid the confounding with <span class="math inline">\(\mathbf{x}_L\)</span>).</li>
<li><span class="math inline">\(\pi(x \mid \boldsymbol\theta_c, \boldsymbol\theta_1, \boldsymbol\theta_2, \phi) = \pi_L(\mathbf{x} \mid \boldsymbol\theta_c, \boldsymbol\theta_1, \phi) \cdot \pi_{ML}(\mathbf{x} \mid \boldsymbol\theta_c, \boldsymbol\theta_2, \phi)\)</span>, which redefines the precision structure (and therefore the conditional behaviour): <span class="math inline">\(\mathbf{Q}= (1-\phi)\mathbf{Q}_L + \phi\mathbf{Q}_{ML}\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="friction-boundaries-and-barriers-1" class="slide level2 smaller">
<h2>Friction, Boundaries, and Barriers</h2>
<h3 id="examples-with-gaussian-kernel-weights-with-covariates">Examples with Gaussian kernel weights with covariates</h3>
<div class="fragment fade-in">
<p><img data-src="./Figures/CH_mainland.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/CH_mainland_grid.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/CH_mainland_grid_elevation.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/CH_mainland_grid_elevation_hlight.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/Q_leroux.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/Q_cov_modified.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/Q_cov_modified2.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
</section>
<section id="friction-boundaries-and-barriers-2" class="slide level2 smaller">
<h2>Friction, Boundaries, and Barriers</h2>
<h3 id="examples-with-fixed-barriers-models-and-varying-permeability">Examples with fixed Barriers models and varying permeability</h3>
<div class="fragment fade-in">
<p><img data-src="./Figures/Plot_label_locations.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/Q_hard_barrier.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/Q_permeable_barrier.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
<div class="fragment fade-in">
<p><img data-src="./Figures/Q_soft_barrier.png" class="absolute" style="top: 140px; left: 30px; width: 1000px; height: 600px; "></p>
</div>
</section>
<section id="friction-boundaries-and-barriers-3" class="slide level2 smaller">
<h2>Friction, Boundaries, and Barriers</h2>
<h3 id="compositional-spatial-data">Compositional spatial data</h3>
<p><img data-src="./Figures/Example_CoDa_SpT.gif" class="fragment fade-in-then-out absolute" style="top: 150px; right: 75px; width: 1020px; height: 580px; "></p>
</section>
<section id="friction-boundaries-and-barriers-4" class="slide level2 smaller">
<h2>Friction, Boundaries, and Barriers</h2>
<h3 id="downscaling-models-in-coda">Downscaling models in CoDa</h3>
<p><strong>Downscaling models for compositional data (CoDa)</strong> can applied to <strong>land-use data</strong> to provide <strong>fine-scale spatial information</strong> while addressing issues from changing areal units, such as evolving <strong>NUTS regions over time</strong> , e.g.&nbsp;land use data from the <strong>European LAMASUS project</strong>. These models define <strong>spatial dependence on a continuous domain</strong> using a <strong>continuous Gaussian random field</strong>, which is then integrated over each area to produce <strong>area-level estimates</strong>. Land-use compositions are modeled through an <strong>additive log-ratio (ALR) transformation</strong> across <strong>five land-use categories</strong>, resulting in the following model specification. <span class="math display">\[
\small
\begin{array}{c}
\mathbf{Y} \sim \text{MVN}(\boldsymbol\mu, \tau_e), \quad \mathbf{Y} = (\mathbf{y}_1 \mid \dots \mid \mathbf{y}_4), \;\boldsymbol\mu = (\boldsymbol\mu_1 \mid \dots \mid \boldsymbol\mu_1), \\
\boldsymbol\mu_1 = \beta_{01}\mathbf{1} + \mathbf{X}_i\boldsymbol\beta_1 + \mathbf{u}_{1s}(\mathbf{s}) + \mathbf{u}_{1t}(t) + \mathbf{u}, \\
\boldsymbol\mu_2 = \beta_{02}\mathbf{1} + \mathbf{X}_i\boldsymbol\beta_2 + \mathbf{u}_{2s}(\mathbf{s}) + \mathbf{u}_{2t}(t) + \mathbf{u}, \\
\boldsymbol\mu_3 = \beta_{03}\mathbf{1} + \mathbf{X}_i\boldsymbol\beta_3 + \mathbf{u}_{3s}(\mathbf{s}) + \mathbf{u}_{3t}(t) + \mathbf{u}, \\
\boldsymbol\mu_4 = \beta_{04}\mathbf{1} + \mathbf{X}_i\boldsymbol\beta_4 + \mathbf{u}_{4s}(\mathbf{s}) + \mathbf{u}_{4t}(t) + \mathbf{u}. \\
\end{array}
\]</span> In this formulation, the spatial component <span class="math inline">\(\mathbf{u}_{js}\)</span> for the <span class="math inline">\(j\)</span>-th log-ratio component is defined through a downscaling procedure, whereby a continuous Gaussian field, represented using the SPDE–FEM approach, is integrated over each <span class="math inline">\(i\)</span>-th area: <span class="math display">\[
u_{ji} = \frac{\int_{\mathbf{s} \in C_i}u_j(\mathbf{s})d\mathbf{s}}{|C_i|}.
\]</span></p>
</section>
<section class="slide level2">

<p><img data-src="./Figures/Downscaling_CoDa.png" class="absolute" style="top: 50px; left: 0px; width: 1400px; "></p>
</section></section>
<section>
<section id="dealing-with-high-complexity-1" class="title-slide slide level1 smaller center">
<h1>Dealing with high complexity</h1>
<ul>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Introduction
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Spatial and Spatio-Temporal Models and Data
</li>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Dealing with high complexity</strong>
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Updating model information
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Distributed and Recursive Inference
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Implementations
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Conclusions
</li>
</ul>
</section>
<section id="dealing-with-high-complexity-2" class="slide level2 smaller">
<h2>Dealing with high complexity</h2>
<p>The term <strong>complexity</strong> is associated with very different meanings, even within the fields of mathematics, computer science, and physics. Here, we adopt a practical or operational interpretation of <strong>computational complexity</strong> in the context of statistical inference. From an operational perspective, the complexity of an inferential process refers to the difficulty of carrying out a task in terms of its computational procedure. This difficulty may arise from different sources and can have fundamentally different impacts on the inferential model.</p>
<p>In particular, we focus on several aspects of computational complexity related to computational cost:</p>
<ul>
<li><strong>Computation time</strong>: the computational cost measured by the time required to fit a model.</li>
<li><strong>Operations per processor core</strong>: the workload in terms of the number of operations (<em>FLOPs</em>) required per processor core.</li>
<li><strong>Memory usage (RAM)</strong>: the amount of memory required to perform the inference.</li>
</ul>
<p>These aspects of computational complexity in statistical analysis are closely interrelated. For instance, an increase in the number of operations per core typically leads to longer computation times, while lower memory requirements often result in reduced computational time. Likewise, a smaller number of operations per core usually implies lower memory usage.</p>
</section>
<section id="dealing-with-high-complexity-3" class="slide level2 smaller">
<h2>Dealing with high complexity</h2>
<p>Computational complexity is encoded in a statistical model through its various components and through the inferential procedure itself. Assuming a fixed model and the INLA methodology, at least four key aspects can be identified as having a direct impact on the computational complexity of inference:</p>
<ul>
<li><strong>Number of observations</strong>: the number of observations determines the dimension of the likelihood <span class="math display">\[
\pi(\mathbf{y} \mid \boldsymbol\eta, \boldsymbol\theta) = \prod_{i=1}^n\pi(y_i \mid \eta_i, \boldsymbol\theta).
\]</span></li>
<li><strong>Dimension of the latent field</strong>: the dimension of the latent field prior—and in particular of its posterior—directly affects both the number of required operations and memory usage, and therefore the overall computational cost: <span class="math display">\[
\small
\log\tilde{\pi}(\boldsymbol\theta \mid \mathbf{y}) = \left[\sum_{i=1}^n\log\pi(y_i \mid \mathbf{x}, \boldsymbol\theta) + |\mathbf{Q}(\boldsymbol\theta)| - \frac{1}{2} (\mathbf{x} - \boldsymbol\mu)^\top \mathbf{Q}(\boldsymbol\theta) (\mathbf{x} - \boldsymbol\mu) - |\mathbf{Q}_{x\mid y, \boldsymbol\theta}(\boldsymbol\theta)| + \frac{1}{2} (\mathbf{x} - \boldsymbol\mu_{x\mid y, \boldsymbol\theta})^\top \mathbf{Q}_{x\mid y, \boldsymbol\theta}(\boldsymbol\theta) (\mathbf{x} - \boldsymbol\mu_{x\mid y, \boldsymbol\theta}) \right]_{\mathbf{x} = \mathbf{x}^*}.
\]</span></li>
<li><strong>The geometry (shape) of the posterior distribution</strong>: the (actual) geometry of the posterior distribution influences the optimization procedure used to locate the mode of the conditional posterior of the latent field, <span class="math display">\[
\arg\min_{\mathbf{x}}-\log\pi(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y}) = \arg\min_{\mathbf{x}} \{-\log\pi(\mathbf{y} \mid \mathbf{x}, \boldsymbol\theta) - \log\pi(\mathbf{x} \mid \boldsymbol\theta)\}.
\]</span></li>
<li><strong>The initial point for optimization</strong>: the choice of the initial point for minimizing the negative log joint posterior of the hyperparamters, or equivalently the negative log conditional posterior, affects the number of operations required to reach the posterior mode.</li>
</ul>
</section></section>
<section>
<section id="updating-model-information" class="title-slide slide level1 center">
<h1>Updating model information</h1>
<ul>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Introduction
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Spatial and Spatio-Temporal Models and Data
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Dealing with high complexity
</li>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Updating model information</strong>
</li>
<ul>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Bayesian Feedback</strong>
</li>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Sequential Consensus</strong>
</li>
</ul>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Distributed and Recursive Inference
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Implementations
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Conclusions
</li>
</ul>
</section>
<section id="section-2" class="slide level2 smaller">
<h2></h2>
<p>The updating of model information refers to a procedure capable of transferring information across models. In Bayesian statistics, Bayes’ theorem provides a natural mechanism for such information transfer: <span class="math display">\[
\pi(\boldsymbol\xi \mid \mathbf{y}) = \frac{\pi(\mathbf{y} \mid \boldsymbol\xi) \pi(\boldsymbol\xi)}{\pi(\mathbf{y})}.
\]</span> This expression can, in general, be reformulated as: <span class="math display">\[
\pi(\boldsymbol\xi \mid \mathbf{y}) \propto
\left\{\begin{array}{l}
\pi(\mathbf{y}_1 \mid \boldsymbol\xi) \cdot \left[ \pi(\mathbf{y}_2 \mid \boldsymbol\xi) \cdot \pi(\boldsymbol\xi)\right] \propto \pi(\mathbf{y}_1 \mid \boldsymbol\xi) \pi(\boldsymbol\xi \mid \mathbf{y}_2) \\
\left[\pi(\mathbf{y}_1 \mid \boldsymbol\xi) \cdot \pi(\boldsymbol\xi)^{1/2} \right] \cdot \left[\pi(\mathbf{y}_2 \mid \boldsymbol\xi) \cdot \pi(\boldsymbol\xi)^{1/2}\right] \propto \pi(\boldsymbol\xi \mid \mathbf{y}_1) \cdot \pi(\boldsymbol\xi \mid \mathbf{y}_2)
\end{array}\right. \quad .
\]</span></p>
<p>However, this formulation raises several fundamental computational and methodological questions:</p>
<ul>
<li><strong>Marginal likelihood computation</strong>: how can the marginal likelihood <span class="math inline">\(\pi(\mathbf{y}_i)\)</span> be computed efficiently, or avoided altogether, in practice?</li>
<li><strong>Closed-form posteriors</strong>: under what conditions is a closed-form expression for the partial posteriors <span class="math inline">\(\pi(\boldsymbol\xi \mid \mathbf{y}_i)\)</span> available, and how should inference proceed when it is not?</li>
<li><strong>Sequential ordering</strong>: when closed-form expressions are unavailable, how does the ordering of datasets in a recursive or sequential update affect the resulting posterior?</li>
<li><strong>Consistency of inference</strong>: to what extent do different orderings and partitions of the data and the model lead to different inferential outcomes, and how can the resulting variability be controlled or mitigated?</li>
</ul>
</section>
<section id="bayesian-feedback" class="slide level2 smaller">
<h2>Bayesian Feedback</h2>
<p><em>Bayesian Feedback</em> refers to a procedure for transferring information between models that share the same parameters but are associated with different observations of the same underlying phenomenon, <span class="math inline">\(\mathcal{M}(\mathbf{y}_1,\boldsymbol\xi)\)</span> and <span class="math inline">\(\mathcal{M}(\mathbf{y}_2, \boldsymbol\xi)\)</span>. That is, it focuses on updating the model parameters <span class="math inline">\(\boldsymbol\xi\)</span> of <span class="math inline">\(\mathcal{M}\)</span>.</p>
<p>The <em>Bayesian Feedback</em> procedure is characterized by defining prior distributions from the marginal posterior distributions of the parameters, thereby emphasizing the transfer of marginal information: <span class="math display">\[
\pi(\xi \mid \mathbf{y}_2) \propto \pi(\mathbf{y}_2 \mid \boldsymbol\xi) \prod_{i=1}^{n}\pi(\xi_i \mid \mathbf{y}_1).
\]</span></p>
<p>However, despite enabling information transfer across models, this approach presents several limitations:</p>
<ul>
<li>It requires the marginal posterior distributions associated with nodes of the latent field to be Gaussian in order for the transfer to be effective within <code>R-INLA</code>.</li>
<li>Correlation between parameters is lost when information is transferred solely through marginal distributions. When the latent field contains sets of nodes with dependencies or correlations, marginal-based transfer cannot be used, as the dependence structure is not preserved in the new model.</li>
<li>It inevitably discards information by transferring only marginal summaries, ignoring joint and higher-order dependence structures.</li>
<li>It is not order independent.</li>
</ul>
</section>
<section class="slide level2">

<div class="fragment fade-in">
<p><img data-src="./Figures/Scheme_BF.png"></p>
</div>
</section>
<section class="slide level2">

<div class="fragment fade-in">
<p><img data-src="./Figures/Diagram_BF.png" class="absolute" style="top: 125px; "></p>
</div>
</section>
<section id="sequential-consensus" class="slide level2 smaller">
<h2>Sequential Consensus</h2>
<p>To overcome the limitations of <em>Bayesian Feedback</em>–particularly those related to the transfer of information from correlated latent field nodes–and to enable a more flexible approach to sequential updating of model parameters, a sequential consensus approach is proposed within the INLA framework. Specifically, two algorithms are introduced: one for general sequential parameter updating, and another tailored to Big Data settings.</p>
<p>The general idea behind both algorithms is that we can perform an update of the fixed effects of the latent field and hyperparamters by the <em>Bayesian Feedback</em> procedure and then a consensus of the latent field nodes related to random effects. Therefore, for the fixed and hyperparameters the sequential update along a partitions of the data <span class="math inline">\(\mathbf{y} = \{\mathbf{y}_1, \dots, \mathbf{y}_n\}\)</span> is perform as: <span class="math display">\[
\begin{array}{c}
\pi(x_k \mid \mathbf{y}) \approx C_k \cdot \pi(\mathbf{y}_1 \mid x_k) \prod_{i=2}^n \pi(x_k \mid \mathbf{y}_i), \\
\pi(\theta_m \mid \mathbf{y}) \approx C_m \cdot \pi(\mathbf{y}_1 \mid \theta_m) \prod_{i=2}^n \pi(\theta_m \mid \mathbf{y}_i).
\end{array}
\]</span></p>
<p>Once the sequential update is perform, it is possible to apply one of the following strategies to perform the consensus for the random effects: (i) marginal weighted averages and (ii) product of multivariate Gaussian densities. <span class="math display">\[
\begin{array}{c}
x_k \approx \sum_{i=1}^n w_i x_{ki} \quad (\text{marginal weighted averages}), \\
\pi(\mathbf{x}_k \mid \mathbf{y}) \approx \prod_{i=1}^n \pi(\mathbf{x}_k \mid \mathbf{y}_i) \quad (\text{product of multivariate Gaussian densities}).
\end{array}
\]</span></p>
</section>
<section id="sequential-consensus-1" class="slide level2 smaller">
<h2>Sequential Consensus</h2>
<h3 id="scheme-of-the-algorithms">Scheme of the algorithms</h3>
<p><img data-src="./Figures/Sequential_Consensus.png" class="absolute" style="left: 200px; width: 900px; height: 600px; "></p>
</section></section>
<section>
<section id="distributed-and-recursive-inference" class="title-slide slide level1 center">
<h1>Distributed and Recursive Inference</h1>
<ul>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Introduction
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Spatial and Spatio-Temporal Models and Data
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Dealing with high complexity
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Updating model information
</li>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Distributed and Recursive Inference</strong>
</li>
<ul>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Distributed Inference (DI)</strong>
</li>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Recursive Inference (RI)</strong>
</li>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Partitioning model structures</strong>
</li>
<li style="color: red; font-synthesis-weight: auto;">
<strong>Combining Distributed and Recursive Inference</strong>
</li>
</ul>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Implementations
</li>
<li style="color: rgba(0,0,0,0.25); font-style: italic;">
Conclusions
</li>
</ul>
</section>
<section id="section-3" class="slide level2 smaller">
<h2></h2>
<p>In this section, we present the proposed distributed and recursive inference approaches, which address many of the main limitations of <em>Bayesian Feedback</em> and <em>Sequential Consensus</em> algorithms. In particular, the strategies developed for both approaches are based on decomposing the inferential process into two main pieces: (i) obtaining an approximation to the joint marginal posterior distribution of the hyperparameters, together with an appropriate integration scheme, and (ii) estimating the posterior distribution of the latent field.</p>
<p>Specifically, the focus is on approximating the <strong>joint marginal posterior of the hyperparameters</strong>, <span class="math display">\[
\tilde{\pi}(\boldsymbol\theta \mid \mathbf{y}) \propto \left[\frac{\pi(\mathbf{y} \mid \mathbf{x}, \boldsymbol\theta) \pi(\mathbf{x} \mid \boldsymbol\theta) \pi(\boldsymbol\theta)}{\tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y})}\right],
\]</span> from which an integration scheme is constructed to obtain the <strong>marginal posterior distributions for each node of the latent field</strong>, <span class="math display">\[
\tilde{\pi}(x_i \mid \mathbf{y}) = \sum_{k=1}^K \tilde{\pi}(x_i \mid \boldsymbol\theta^k, \mathbf{y}) \tilde{\pi}(\boldsymbol\theta^k \mid \mathbf{y})\Delta_k.
\]</span></p>
<p>To achieve this, it is necessary to define how to compute the <strong>conditional posterior of the latent field</strong> <span class="math inline">\(\tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y})\)</span>, in a distributed and recursive setting, as well as how to obtain its marginal distributions <span class="math inline">\(\tilde{\pi}(x_i \mid \boldsymbol\theta, \mathbf{y})\)</span>.</p>
<p>Once these components are available, a range of strategies can be employed to compute the <strong>marginal posterior distributions of the hyperparameters</strong> (<span class="math inline">\(\pi(\boldsymbol\theta_j \mid \mathbf{y})\)</span>) and to derive appropriate <strong>goodness-of-fit measures</strong>.</p>
</section>
<section id="distributed-inference" class="slide level2">
<h2>Distributed Inference</h2>
<p>Following the same structure used to present the methodological details of INLA, we adopt this framework to describe the proposed approach for <strong>distributed inference</strong> based on <strong>INLA</strong>. In both frameworks, the aim is to perform inference in settings where the dataset is either naturally partitioned or deliberately split into multiple subsets <span class="math inline">\(\mathbf{y} = \{\mathbf{y}_1, \dots, \mathbf{y}_n\}\)</span>. Accordingly, we examine the method in terms of the computation of:</p>
<ul>
<li>the conditional posterior of the latent field,</li>
<li>the joint marginal posterior of the hyperparameters,</li>
<li>the marginal posterior of the latent field, and</li>
<li>the marginal posterior of the hyperparameters.</li>
</ul>
</section>
<section id="section-4" class="slide level2 smaller">
<h2></h2>
<h3 id="conditional-posterior-of-the-latent-field-1">Conditional posterior of the latent field</h3>
<p>To compute the conditional posterior of the latent field when the data have been partitioned, we can derive it from the general expression: <span class="math display">\[
\pi(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y}) \propto \prod_{i=1}^n \left[ \pi(\mathbf{y}_i \mid \mathbf{x}, \boldsymbol\theta) \pi(\mathbf{x} \mid \boldsymbol\theta)^{w_i}\right] \propto \left(\prod_{i=1}^n \left[ \pi(\mathbf{y}_i \mid \mathbf{x}, \boldsymbol\theta) \pi(\mathbf{x} \mid \boldsymbol\theta)\right]\right)/\pi(\mathbf{x} \mid \boldsymbol\theta)^{n-1},
\]</span> where <span class="math inline">\(\sum_{i=1}^n w_i = 1\)</span> and <span class="math inline">\(w_i&gt;0\)</span>. This formula allow us to proposed two strategies to compute the conditional posterior: (i) scaling the prior distribution and (ii) the scaling the posterior.</p>
<p>In general, the expression to compute the conditional posterior distribution in a distributed framework is: <span class="math display">\[
\tilde{\pi}_d(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y}) \propto \prod_{i=1}^n \tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y}_i),
\]</span> where <span class="math inline">\(\pi(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y}_i)\)</span> is the Gaussian approximation for the conditional posterior of the latent field for the i-th partition. Therefore, the conditional posterior under the distributed framework is also a Gaussian distribution with mean (<span class="math inline">\(\boldsymbol\mu_d(\boldsymbol\theta)\)</span>) and precision matrix (<span class="math inline">\(\mathbf{Q}_d(\boldsymbol\theta)\)</span>) defined as: <span class="math display">\[
\begin{array}{c}
\mathbf{Q}_d(\boldsymbol\theta) = \sum_{i=1}^n \mathbf{Q}_i(\boldsymbol\theta) = \sum_{i=1}^n \left[ w_i\mathbf{Q}(\boldsymbol\theta) - \mathbf{A}_i^\top \nabla_{\boldsymbol\eta_i}^2\pi(\mathbf{y}_i \mid \boldsymbol\eta_i, \boldsymbol\theta) \mathbf{A}_i  \right]_{\mathbf{\mathbf{x}=\mathbf{x}_i^*}}, \\
\boldsymbol\mu_d(\boldsymbol\theta) = \mathbf{Q}_d(\boldsymbol\theta)^{-1} \times \sum_{i=1}^n\mathbf{Q}_i(\boldsymbol\theta)\boldsymbol\mu_i(\boldsymbol\theta), \quad \boldsymbol\mu_i(\boldsymbol\theta) = \mathbf{x}_i^* = -\arg\min_{\mathbf{x}}\left\{ - \log\pi(\mathbf{y}_i \mid \mathbf{x}, \boldsymbol\theta) - w_i\log\pi(\mathbf{x} \mid \boldsymbol\theta) \right\}.
\end{array}
\]</span></p>
</section>
<section id="section-5" class="slide level2 smaller">
<h2></h2>
<h3 id="joint-marginal-posterior-of-the-hyperparameters">Joint marginal posterior of the hyperparameters</h3>
<p>The joint marginal posterior of the hyperparameters, when the dataset is partitioned, can be computed as the product of the joint marginal posteriors of the hyperparameters obtained via the INLA method for each of the partitions. That is: <span class="math display">\[
\tilde{\pi}_{d}(\boldsymbol\theta \mid \mathbf{y}) \propto \prod_{i=1}^n \left[\frac{\pi(\mathbf{y}_i \mid \mathbf{x}, \boldsymbol\theta)\pi(\mathbf{x} \mid \boldsymbol\theta)\pi(\boldsymbol\theta)}{\tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta, \mathbf{y}_i)}\right]_{\mathbf{x}=\mathbf{x}_i^*} = \prod_{i=1}^n \tilde{\pi}(\boldsymbol\theta \mid \mathbf{y}_i).
\]</span> One of the main problems is on computing the distributed proposed joint marginal to lately defined the CCD scheme, or other scheme, for computing the marginals of the latent field, along with the marginal likelihood and other measures. In this sense, the proposal is to leverage the Gaussian approximation that is possible to compute for each partition and defined the joint maginal posterior as: <span class="math display">\[
\tilde{\pi}_{dG}(\boldsymbol\theta \mid \mathbf{y}) \propto \prod_{i=1}^n \tilde{\pi}_G(\boldsymbol\theta \mid \mathbf{y}_i).
\]</span> From this estimation we have an approximate posterior density for the hyperparameters (in their internal parametrization) that allow us to build a integration scheme, like the CCD. Therefore, once we have the integration scheme leveraging the Gaussian approximation we can use the former formula to compute the joint marginal posterior of the hyperparameters in the support points (integration points) for the marginal posterior of the laten field nodes.</p>
</section>
<section id="section-6" class="slide level2 smaller">
<h2></h2>
<h3 id="marginal-posterior-of-the-latent-field-1">Marginal posterior of the latent field</h3>
<p>In the computation of the marginal posterior for each node of the latent field we leverage the formula used in the INLA method: <span class="math display">\[
\tilde{\pi}(x_i \mid \mathbf{y}) = \sum_{k=1}^K \tilde{\pi}(x_i \mid \boldsymbol\theta^k, \mathbf{y}) \tilde{\pi}(\boldsymbol\theta^k \mid \mathbf{y}) \Delta_k,
\]</span> where we apply a numerical integration approach using the integration scheme fixing a set of support points <span class="math inline">\(\{\boldsymbol\theta^k\}_{k=1}^K\)</span>, <span class="math inline">\(\tilde{\pi}(x_i \mid \boldsymbol\theta^k, \mathbf{y})\)</span> and <span class="math inline">\(\tilde{\pi}(\boldsymbol\theta^k \mid \mathbf{y})\)</span> are the marginal of the conditional of the latent field and the joint marginal of the hyperparatmers at the support points, and <span class="math inline">\(\Delta_k\)</span> are the weights for each support point.</p>
<p>This expression can be easily rewritten, in a strightforward manner, for the distributed approach as: <span class="math display">\[
\tilde{\pi}_d(x_i \mid \mathbf{y}) = \sum_{k=1}^K \tilde{\pi}_d(x_i \mid \boldsymbol\theta^k, \mathbf{y}) \tilde{\pi}_d(\boldsymbol\theta^k \mid \mathbf{y}) \Delta_k,
\]</span> where the <span class="math inline">\(\Delta_k\)</span> quantity is compute, and generally can be computed, in the same way as in the standard INLA approach, <span class="math inline">\(\tilde{\pi}_d(\boldsymbol\theta^k \mid \mathbf{y})\)</span> is the marginal computed in the distributed proposal previously explained and <span class="math inline">\(\tilde{\pi}_d(x_i \mid \boldsymbol\theta^k, \mathbf{y})\)</span> is the marginal of the conditional posterior of the latent field for each node. This later quantity is the only that we need to compute now to perform the calculus of the marginal for each node.</p>
</section>
<section id="section-7" class="slide level2 smaller">
<h2></h2>
<h3 id="marginal-posterior-hyperparameters">Marginal posterior hyperparameters</h3>
<p>The marginal posterior of the hyperparameters can be computed following the same strategies as in the based INLA method:</p>
<ul>
<li>Marginalized a Gaussian approximation around the mode of the joint marginal posterior.</li>
<li>Numerical integration using a exploration grid and interpolation: (i) direct interpolation, or (ii) interpolation using an asymmetric Gaussian apprximation.</li>
<li>Numerical integration-free algorithm (default in <code>R-INLA</code>).</li>
</ul>
<p>In this setting, the distributed expression for the joint marginal posterior of the hyperparameters must be used as the basis for deriving the marginal posterior of each hyperparameter: <span class="math display">\[
\tilde{\pi}_d(\boldsymbol\theta \mid \mathbf{y}) \propto \prod_{i=1}^n \tilde{\pi}(\boldsymbol\theta \mid \mathbf{y}_i).
\]</span></p>
<p>However, within a distributed framework, accurately computing marginal posterior distributions based on this expression is computationally demanding. This difficulty arises both from the numerical integration strategies required and from the additional computational burden imposed by distributed implementations of integration-free algorithms. Consequently, the simplest and most computationally efficient approach is to compute the marginal posteriors using the Gaussian approximation, <span class="math inline">\(\pi_{dG}(\boldsymbol\theta \mid \mathbf{y})\)</span>.</p>
</section>
<section id="recursive-inference" class="slide level2">
<h2>Recursive Inference</h2>
<p>Following the structure used to present the methodological details of INLA, we describe the proposed recursive inference approach based on INLA, building on the distributed framework introduced above. The discussion therefore focuses on:</p>
<ul>
<li>the <strong>computation of the conditional posterior of the latent field</strong> and</li>
<li>the <strong>marginal posterior of the latent field</strong>.</li>
</ul>
<p>The joint marginal and marginal posterior distributions of the hyperparameters are omitted, as they are obtained using the same strategies as in the distributed setting. Notably, any distributed framework can be applied recursively, allowing the distributed inference approach to be used directly for recursive inference.</p>
</section>
<section id="section-8" class="slide level2 smaller">
<h2></h2>
<h3 id="conditional-posterior-latent-field">Conditional posterior latent field</h3>
<p>To compute the conditional posterior of the latent field in a recursive setting, we exploit the fact that, once the set of support points is fixed, the computation can be carried out sequentially as <span class="math display">\[
\tilde{\pi}_r(\mathbf{x} \mid \boldsymbol\theta^k, \mathbf{y}_{1:i}) = \tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta^k, \mathbf{y}_{1:i}).
\]</span> That is, at the <span class="math inline">\(i\)</span>-th step, the Gaussian approximation to the conditional posterior is defined recursively. Consequently, the posterior distribution <span class="math inline">\(\tilde{\pi}_r(\mathbf{x} \mid \boldsymbol\theta^k, \mathbf{y}_{1:i})\)</span> is fully characterized by a mean vector <span class="math inline">\(\mathbf{x}_{ir}(\boldsymbol\theta^k)\)</span> and a precision matrix <span class="math inline">\(\mathbf{Q}_{ir}(\boldsymbol\theta^k)\)</span>, given by <span class="math display">\[
\begin{array}{c}
\boldsymbol\mu_{ir}(\boldsymbol\theta^k) = \mathbf{x}^*_{ir} = -\arg\min_{\mathbf{x}}\{-\log\pi(\mathbf{y}_{i} \mid \mathbf{x}, \boldsymbol\theta^k) -\log\tilde{\pi}_{r}(\mathbf{x} \mid \boldsymbol\theta^k, \mathbf{y}_{1:(i-1)}) \} , \\
\mathbf{Q}_{ir}(\boldsymbol\theta^k) = \mathbf{Q}_{(i-1)r}(\boldsymbol\theta^k) - \left[\mathbf{A}^\top_i \nabla^2_{\boldsymbol\eta_i}\log\pi(\mathbf{y}_{i} \mid \boldsymbol\eta, \boldsymbol\theta^k) \mathbf{A}_i \right]_{\mathbf{x}=\mathbf{x}^*_{ir}}.
\end{array}
\]</span></p>
<p>Naturally, at the first step of the recursive algorithm, the prior distribution coincides with the standard prior used for the model, <span class="math inline">\(\pi(\mathbf{x} \mid \boldsymbol\theta)\)</span>.</p>
<p>Under this procedure, it becomes evident that the proposed approach is, in general, not independent of the ordering adopted for the recursive inference.</p>
</section>
<section id="section-9" class="slide level2 smaller">
<h2></h2>
<h3 id="marginal-posterior-of-the-latent-field-2">Marginal posterior of the latent field</h3>
<p>The marginal posterior of the latent field is computed by directly applying one of the four standard INLA strategies:</p>
<ul>
<li>marginalizing the Gaussian approximation of the conditional posterior evaluated at the mode,</li>
<li>applying a Laplace approximation independently for each node,</li>
<li>using a simplified Laplace approximation based on a third-order Taylor expansion, or</li>
<li>using the Gaussian approximation with a Variational Bayes (VB) correction to the mean.</li>
</ul>
<p>Accordingly, the marginal posterior distributions within the recursive framework are obtained as <span class="math display">\[
\tilde{\pi}_r(x_i \mid \mathbf{y}_{1:i}) = \sum_{k=1}^K \tilde{\pi}_r(x_i \mid \boldsymbol\theta^k, \mathbf{y}_{1:i})\tilde{\pi}_r(\boldsymbol\theta^k \mid \mathbf{y}_{1:i})\Delta_k.
\]</span></p>
<p>A key feature of the recursive framework is that the marginal of the conditional posterior of the latent field <span class="math inline">\(\tilde{\pi}(x_i \mid \boldsymbol\theta^k, \mathbf{y}_{1:i})\)</span> can be computed using the standard INLA methodology without modification. The only difference from the base INLA approach is that, at each recursive step and for each support point <span class="math inline">\(\boldsymbol\theta^k\)</span>, the prior distribution of the latent field is given by <span class="math inline">\(\tilde{\pi}_G(\mathbf{x} \mid \boldsymbol\theta^k,  \mathbf{y}_{1:(i-1)})\)</span>.</p>
</section>
<section id="partitioning-model-structures" class="slide level2 smaller">
<h2>Partitioning model structures</h2>
<p>In this case, the key condition imposed on the partitioning of the latent field—either in the distributed framework or along the sequence in the recursive framework—is that the resulting <strong>prior distributions must be recoverable</strong>. To achieve this, two strategies can be proposed, both <strong>based on partitioning the precision matrix</strong> of the prior distribution: (i) partitioning the latent field into diagonal blocks, which entails a loss of correlation information between blocks, or (ii) extending the diagonal blocks by scaling those elements that appear in multiple partitions.</p>
<p>Formally, <span class="math display">\[
\pi(\mathbf{x} \mid \boldsymbol\theta) = \prod_{i=1}^n\pi(\mathbf{x}_i \mid \boldsymbol\theta),
\]</span> where <span class="math inline">\(\pi(\mathbf{x}_i \mid \boldsymbol\theta)=\text{MVN}(\boldsymbol\mu_i, \mathbf{Q}_i(\boldsymbol\theta))\)</span>. Consequently, we obtain <span class="math display">\[
\begin{array}{c}
\mathbf{Q}_p(\boldsymbol\theta) = \sum_{i=1}^n\mathbf{Q}_i(\boldsymbol\theta), \\
\boldsymbol\mu_p = \mathbf{Q}_p(\boldsymbol\theta)^{-1} \sum_{i=1}^n \mathbf{Q}_i(\boldsymbol\theta) \boldsymbol\mu_i.
\end{array}
\]</span></p>
<p><strong>This formulation</strong> enables the construction of a distributed and recursive framework in which the <strong>computational burden</strong> associated with evaluating the posterior densities of the latent field <strong>can be substantially reduced</strong>, particularly for <strong>spatial</strong>, <strong>temporal</strong>, and especially <strong>spatio-temporal models</strong>.</p>
</section>
<section id="section-10" class="slide level2 smaller">
<h2></h2>
<h3 id="partitioning-algorithms">Partitioning algorithms</h3>
<p>The fundamental basis for defining partitioning algorithms lies in the precision matrices associated with the selected components of the latent field to be partitioned—whether spatial, temporal, spatio-temporal, or otherwise. To this end, four algorithms are proposed for carrying out such partitions:</p>
<div class="fragment highlight-current-red" data-fragment-index="1">
<div class="fragment semi-fade-out" data-fragment-index="2">
<ul>
<li><strong>Partitioning by separators</strong> (nested dissection),</li>
</ul>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="1">
<p><img data-src="./Figures/Partitioning_Alg1.png" class="absolute" style="top: 170px; left: 600px; width: 550px; "></p>
</div>
<div class="fragment highlight-current-red" data-fragment-index="2">
<div class="fragment semi-fade-out" data-fragment-index="3">
<ul>
<li><strong>Block-diagonal partitioning</strong> (bandwidth reduction),</li>
</ul>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="2">
<p><img data-src="./Figures/Partitioning_Alg2.png" class="absolute" style="top: 180px; left: 580px; width: 700px; "></p>
</div>
<div class="fragment highlight-current-red" data-fragment-index="3">
<div class="fragment semi-fade-out" data-fragment-index="4">
<ul>
<li><strong>Spectral clustering for graph partitioning</strong>, and</li>
</ul>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="3">
<p><img data-src="./Figures/Partitioning_Alg3.png" class="absolute" style="top: 180px; left: 580px; width: 700px; "></p>
</div>
<div class="fragment highlight-current-red" data-fragment-index="4">
<ul>
<li><strong>Block partitioning via Voronoi tessellation</strong>.</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<p><img data-src="./Figures/Partitioning_Alg4.png" class="absolute" style="top: 180px; left: 580px; width: 700px; "></p>
</div>
</section>
<section id="combining-distributed-and-recursive-inference" class="slide level2">
<h2>Combining Distributed and Recursive Inference</h2>
<p><img data-src="./Figures/Combined_DI_and_RI.png" class="absolute" style="top: 175px; "></p>
</section></section>
<section>
<section id="conclusions" class="title-slide slide level1 center">
<h1>Conclusions</h1>

</section>
<section id="conclusions-1" class="slide level2 smaller">
<h2>Conclusions</h2>
<p>Spatial and spatio-temporal models are a cornerstone for analyzing data across many fields. There is significant potential to <strong>improve and innovate modeling approaches</strong>. Today, the large volume of data and the complexity of models also demand <strong>methodological and computational advances</strong> to address challenges such as <strong>information transfer and sharing across statistical models</strong>, enable <strong>efficient Bayesian inference in complex spatio-temporal settings</strong>, and tackle difficulties in the <strong>spatio-temporal analysis of compositional data (CoDa)</strong>.</p>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="Seminar_Vogelwarte_24_02_2026_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1250,

        height: 760,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script type="text/javascript">
      Reveal.on('ready', event => {
        if (event.indexh === 0) {
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
      });
      Reveal.addEventListener('slidechanged', (event) => {
        if (event.indexh === 0) {
          Reveal.configure({ slideNumber: null });
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
        if (event.indexh === 1) { 
          Reveal.configure({ slideNumber: 'c/t' });
          document.querySelector("div.has-logo > img.slide-logo").style.display = null;
        }
      });
    </script>
    

</body></html>